{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**Machine Learning Projects Milestone 2 Report**\n",
        "\n",
        "\\begin{align}\n",
        "  \\text{Alexander Sharpe: sharpeal@bc.edu} \\\\\n",
        "  \\text{Andrea Brigliadori: brigliad@bc.edu} \\\\\n",
        "  \\text{Alan Chang: changwg@bc.edu} \\\\\n",
        "  \\text{Jasroop Dhingra: dhingraj@bc.edu} \\\\\n",
        "  \\text{Cole Dixon: dixonco@bc.edu}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3ovJyPSpJI6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing parts for milestone 2\n",
        "\n",
        "3.1.2) Explain why the data is good to solve the problem\n",
        "\n",
        "Explain which member of the group will do each part.\n",
        "\n",
        "3.5) Explain who has done what between Milestone 1 and Milestone 2 (this is different from 3.4 which asks about who will do what in the future)\n"
      ],
      "metadata": {
        "id": "SqxEB9bj8wTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "Biosensors are becoming an increasingly popular method of detecting specific chemical substances via the combination of a biological component and modern technologies. These sensors can be used in a variety of contexts, one such being the ability to detect proteins in a water source. For example, it may be useful to detect a protein affine to COVID-19, or any other potentially harmful protein, in a source of drinking water. We can employ biosensors in the form of an artificial nucleic acid segment, called an aptamer: each individual protein reacts in a unique way to the aptamer, so certain aptamers can be used on a biosensor to capture specific proteins. The crucial point is the binding affinity between a protein-aptamer pair, which can be measured with kd values, were lower figures indicate better affinity. In a traditional lab setting, testing the binding affinity between one protein and aptamer pair proves too costly in terms of both time and money. This is where machine learning comes in. A database PDBBind containing numerous aptamer-protein combinations and their interactions exists. We plan to use this dataset to train a machine learning model that, given protein-aptamers complexes as input is able to rank them in order of binding affinity. This implicitly helps identify which aptamers bind best to which proteins and may find applications in the search for a simpler and time-saving way to build biosensors able to detect proteins."
      ],
      "metadata": {
        "id": "8RRs5T0ktqfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expectations\n",
        "In our case good behavior would mean reaching a classifier able to rank aptamer-protein interactions with high enough quality. A positive result could also be a prediction considered unexpected, but that can be justified from a biological perspective.\n",
        "A negative situation could be that the classifiers we're able to make don't manage to recognize significant patterns in the data or show a tendency to overfit, resulting in a low accuracy. Or, it could happen that we can obtain sufficient results only at the cost of high inefficiency and long running times. Also, obtaining results that we're unable to interpret or explain may represent a complication."
      ],
      "metadata": {
        "id": "NBhEBlsqtvC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Related Work:  \n",
        "\"Deciphering interaction fingerprints from protein\n",
        "molecular surfaces using geometric deep learning\" by Gazina et al. uses geometric deep learning models to do three tasks:\n",
        "\n",
        "\n",
        "1.   Given the surface and chemical properties of a protein binding site, it can predict how well a ligand (small ligand like metabolites) binds to that binding site.\n",
        "2.   Given the shape and chemical features of the entire protein, it can also predict what the binding site of the protein is. The calculation of the binding site/sites seems to be universal. However, this is still a little unclear.\n",
        "3.   Given a pair of binding sites/interacting surface patches from different proteins, it can score how well these patches bind to each other.\n",
        "\n",
        "Although this paper focuses more on protein-protein interactions, we believe their models can be applied to protein-aptamer interactions.\n",
        "\n",
        "\n",
        "\n",
        "\"AptaNet as a deep learning approach for aptamer–protein interaction prediction\" by Neda Emami and Reza Ferdousi uses a fully-connected neural network to predict whether the binding affinity of a protein-aptamer pair is above a certain threshold. The problem explored in this paper is much simpler than our problem of correctly ranking the binding affinity of different protein-aptamer complexes. Nonetheless, the features that extract and input into their machine learning model could be of use to us.\n",
        "\n",
        "\"AptaBERT: Predicting aptamer binding interactions\" by Morsch et al. attempts to achieve higher accuracy on the problem that AptaNet tackles. It attempts to do so (and seemingly does so based on its empirical data) by using a different neural network model. Before passing its dataset into a fully connected neural network, it passes it through a pre-training model BERT (Bidirectional Encoder Representations from Transformers) developed by Google. Since BERT relies on recurrent neural networks, it encodes each protein-aptamer complex as a sequence of its amino acids and nucleic acids. BERT is perhaps a pre-training model that we can use as well.\n",
        "\n"
      ],
      "metadata": {
        "id": "amwoUp2OyT7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Materials and Methods\n",
        "The Protein Databank used throughout the study contains files that are populated with geometric and chemical properties of each protein. These characteristics will be extracted by means of the molecular dynamics software GROMACS, which enables to create the needed features for training. They will be used in the realization of the training, test, and validation sets. The performance of our model will be evaluated by comparison with a \"ground truth\", consisting in the existing experimental results.\n",
        "\n",
        "Aside from the neural network development, one of the main friction points we will have to study will be which input features determine best how well proteins and aptamers bind. What follows is a comprehensive description of the features in our raw data.\n",
        "\n",
        "\n",
        "\n",
        "ID:\n",
        "\n",
        "  - Identifier for each entry in the dataset.\n",
        "\n",
        "  \n",
        "PDB code:\n",
        "\n",
        "- PDB stands for Protein Data Bank, and the PDB code is an alphanumeric identifier assigned to a specific macromolecular structure in the PDB.\n",
        "\n",
        "\n",
        "Subset:\n",
        "\n",
        "  - Subgroup or category to which the entry belongs. It could be used for grouping similar interactions. This may be a column we want to delete, as it seems that all its entries are 'general'.\n",
        "\n",
        "\n",
        "Complex Type:\n",
        "\n",
        "  - Describes the type of protein-aptamer complex, indicating the nature of the interaction (e.g., binding, enzyme-substrate).\n",
        "\n",
        "\n",
        "Resolution:\n",
        "\n",
        "  - Resolution of the experimental data, indicating the level of detail in the three-dimensional structure of the protein-aptamer complex. 'NMR' (Nuclear Magnetic Resonance) is a technique that provides information about the atomic-level structure of molecules in solution. The numeric values in the column instead are crystallographic resolutions, expressed in Ångströms (Å). Crystallography is another common method used to determine protein structures. The numeric value represents the resolution of the structure, with lower values indicating higher resolution and better-defined structures.\n",
        "\n",
        "\n",
        "Affinity Data:\n",
        "\n",
        "  - Data on the dissociation constant (Kd) of the protein-aptamer interaction, expressed in nanomolars (nM). The dissociation constant is a measure of how tightly an aptamer binds to a protein. A lower Kd value indicates a stronger binding affinity between the two molecules.\n",
        "\n",
        "pKd, pKi, pIC50:\n",
        "\n",
        "  - Measures of binding affinity, expressed as negative logarithms (pKd for dissociation constant, pKi for inhibition constant, pIC50 for half-maximal inhibitory concentration. The 'p' in front of the names stands for 'negative logarithm').\n",
        "\n",
        "Release Year:\n",
        "\n",
        "  - Indicates the year when the data or structure was released or published.\n",
        "\n",
        "Protein Name:\n",
        "\n",
        "  - The name of the protein involved in the interaction.\n",
        "\n",
        "Ligand Name:\n",
        "\n",
        "  - Name of the aptamer in the interaction.\n",
        "\n",
        "Pubmed ID:\n",
        "\n",
        "  - PubMed Identifier, a unique number assigned to each PubMed article (publication associated with the data).\n",
        "\n",
        "Pubchem SID:\n",
        "\n",
        "  - PubChem Substance Identifier, a unique identifier assigned to chemical substances.\n",
        "\n",
        "EC number:\n",
        "\n",
        "  - Enzyme Commission number, a numerical classification for enzymes based on the type of chemical reaction they catalyze.\n",
        "\n",
        "Reference:\n",
        "\n",
        "  - Additional information or details about the source of the data.\n",
        "\n",
        "UniProt AC:\n",
        "\n",
        "  - UniProt Accession Code, a unique identifier assigned to each protein entry in the UniProt database."
      ],
      "metadata": {
        "id": "nCsDlcWS0aPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We proceed with the imports of the necessary packages and the definitions of useful functions, followed by a preliminary data preprocessing."
      ],
      "metadata": {
        "id": "1KA800Xj7EIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports and useful functions"
      ],
      "metadata": {
        "id": "DI6bTIVg1Eih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install scikit-learn numpy pandas umap-learn matplotlib seaborn scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGFV8RbPkDbR",
        "outputId": "16f16562-6b2e-4eb4-a206-89e553a4e7c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Building wheels for collected packages: umap-learn\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=8a7bb3c98dd94b94b327d6736e4d49caa7cbe88f793740f1fda7ad3dcafc022f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "Successfully built umap-learn\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.11 umap-learn-0.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, k_means, SpectralClustering\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay, f1_score, recall_score, silhouette_score\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "state = 42 # Random state to use"
      ],
      "metadata": {
        "id": "5pwpx1uh1GdR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful functions\n",
        "\n",
        "## Data preprocessing\n",
        "\n",
        "def translate_kd(kd):\n",
        "    kd.strip()\n",
        "    kd = kd[3:]\n",
        "    factor = 0\n",
        "    if \"nM\" in kd:\n",
        "        factor = 1\n",
        "    elif \"pM\" in kd:\n",
        "        factor = .001\n",
        "    elif \"uM\" in kd:\n",
        "        factor = 1000\n",
        "    kd = kd[:-2]\n",
        "    try:\n",
        "        return float(kd) * factor\n",
        "    except:\n",
        "        print(kd)\n",
        "        return None\n",
        "\n",
        "def output(df):\n",
        "    df_Final = pd.DataFrame(columns=[\"PDB code 1\",\"Protein Name 1\",\"Ligand Name 1\",\"Affinity Data 1\",\"kd value (nM) 1\",\"PDB code 2\",\"Protein Name 2\",\"Ligand Name 2\",\"Affinity Data 2\",\"kd value (nM) 2\",\"Output\"])\n",
        "    for _ , row1 in df.iterrows():\n",
        "        for _ , row2 in df.iterrows():\n",
        "            if row1[\"PDB code\"] == row2[\"PDB code\"]:\n",
        "                continue\n",
        "            output = 1 if row1[\"kd value (nM)\"] < row2[\"kd value (nM)\"] else 0\n",
        "            temp = row1.to_list() + row2.to_list()\n",
        "            temp.append(output)\n",
        "            df_Final.loc[len(df_Final.index)] = temp\n",
        "    return df_Final\n",
        "\n",
        "## Preparing the data for classification\n",
        "\n",
        "def random_indices(data, test_ratio):\n",
        "    \"\"\"\n",
        "    Returns shuffled indices, useful for reordering the rows of the datasets.\n",
        "    \"\"\"\n",
        "    shuffled_indices = np.random.permutation(len(data))\n",
        "    test_set_size = int(len(data) * test_ratio)\n",
        "    test_indices = shuffled_indices[:test_set_size]\n",
        "    train_indices = shuffled_indices[test_set_size:]\n",
        "    return train_indices, test_indices\n",
        "\n",
        "def split(x, y, rate = 0.2):\n",
        "    \"\"\"\n",
        "    Split train and test.\n",
        "\n",
        "    \"\"\"\n",
        "    train_ind, test_ind = random_indices(x, rate)\n",
        "    x_train = x[train_ind]\n",
        "    x_test = x[test_ind]\n",
        "    y_train = y[train_ind]\n",
        "    y_test = y[test_ind]\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "JrcpQkjO7OPs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "JTo4ZmK0rAey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting spherical coordinates"
      ],
      "metadata": {
        "id": "LhIiPQh8rDt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data with cartesian coordinates converted to spherical coordinates\n",
        "conv_data1 = pd.read_csv(\"Data_Spherical_Coordinates.csv\", delimiter=\"\\s+\", engine='python', skiprows=5, header=None)\n",
        "print(conv_data1.shape)\n",
        "angles = conv_data1.iloc[:,1:3]\n",
        "angles.head"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZrCYDL9nrzEW",
        "outputId": "616eac92-ea53-4a33-bcc2-8bd375918296"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Data_Spherical_Coordinates.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-53a337b4784b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Data with cartesian coordinates converted to spherical coordinates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconv_data1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data_Spherical_Coordinates.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\s+\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_data1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mangles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_data1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mangles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data_Spherical_Coordinates.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=10, n_init=\"auto\", random_state=0)\n",
        "km_clusters = kmeans.fit_predict(angles)\n",
        "km_cluster_centers = kmeans.cluster_centers_\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(angles.iloc[:, 0], angles.iloc[:, 1], c=km_clusters, cmap='viridis', s=5, alpha=0.5)\n",
        "plt.scatter(km_cluster_centers[:, 0], km_cluster_centers[:, 1], marker='o', c='red', s=30, label='Cluster centers')\n",
        "\n",
        "plt.title('KMeans Clustering')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print('Cluster Centers:')\n",
        "print()\n",
        "print(km_cluster_centers)"
      ],
      "metadata": {
        "id": "FtBsQamQsdwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "km_cluster_centers_indeces = []\n",
        "for center in km_cluster_centers:\n",
        "    distances = np.linalg.norm(angles - center, axis=1)\n",
        "    index_closest_point = np.argmin(distances)\n",
        "    km_cluster_centers_indeces.append(index_closest_point)\n",
        "\n",
        "km_representative_data = conv_data1.iloc[km_cluster_centers_indeces, :]"
      ],
      "metadata": {
        "id": "EJ4akmn3sec9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spectral = SpectralClustering(n_clusters=10, assign_labels='cluster_qr', random_state=state, n_neighbors=int(np.sqrt(angles.shape[0])))\n",
        "s_clusters = spectral.fit_predict(angles)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(angles.iloc[:, 0], angles.iloc[:, 1], c=s_clusters, cmap='viridis', s=5, alpha=0.5)\n",
        "\n",
        "s_cluster_centers = np.zeros((10, 2))\n",
        "for i in range(10):\n",
        "    s_cluster_points = angles[s_clusters == i]\n",
        "    s_cluster_centers[i] = np.mean(s_cluster_points, axis=0)\n",
        "plt.scatter(s_cluster_centers[:, 0], s_cluster_centers[:, 1], marker='o', c='red', s=30, label='Cluster centers')\n",
        "\n",
        "plt.title('Spectral Clustering')\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "print('Cluster Centers:')\n",
        "print()\n",
        "print(s_cluster_centers)"
      ],
      "metadata": {
        "id": "5SDbDbwdshmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s_cluster_centers_indeces = []\n",
        "for center in s_cluster_centers:\n",
        "    distances = np.linalg.norm(angles - center, axis=1)\n",
        "    index_closest_point = np.argmin(distances)\n",
        "    s_cluster_centers_indeces.append(index_closest_point)\n",
        "\n",
        "s_representative_data = conv_data1.iloc[s_cluster_centers_indeces, :]"
      ],
      "metadata": {
        "id": "MLUtY9qRsla8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Silhouette score for Kmeans\n",
        "kmeans_silhouette_score = silhouette_score(angles, km_clusters)\n",
        "\n",
        "#Silhouette score for Spectral Clustering\n",
        "spectral_silhouette_score = silhouette_score(angles, s_clusters)\n",
        "\n",
        "print(\"Indice silhouette per K-Means:\", kmeans_silhouette_score)\n",
        "print(\"Indice silhouette per Spectral Clustering:\", spectral_silhouette_score)\n",
        "print()\n",
        "print(\"We choose the representative data obtained with KMeans:\")\n",
        "km_representative_data"
      ],
      "metadata": {
        "id": "XE4P7iJ6swpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of our data preprocessing was to convert the data into a usable format for training models. The first step was to create our labels based off of the experimental kd values that we have. There are 53 different protein-aptamer complexes that we are looking at, and in order to create labels we need to look at every pair of complexes and determine which one has a lower kd value which is associated with a stronger bond. By framing the problem as a comparison, our number of data points becomes 53*52 = 2757 data points."
      ],
      "metadata": {
        "id": "aOKXuza7k-xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"KD_data.csv\")\n",
        "df.drop(index = 0,inplace=True)\n",
        "df = df[['PDB code','Protein Name','Ligand Name','Affinity Data']]\n",
        "df[\"kd value (nM)\"] = [translate_kd(kd) for kd in df[\"Affinity Data\"]]\n",
        "df_compare = pd.read_csv(\"Aptamer-Protein-Information.csv\")\n",
        "df = df[df[\"PDB code\"].isin(df_compare[\"Unnamed: 0\"])]\n",
        "df = output(df)\n",
        "df.to_csv(\"data_output.csv\",index=False)"
      ],
      "metadata": {
        "id": "-YAsSBXflHhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "NzdtD_mh75wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature selection"
      ],
      "metadata": {
        "id": "x4DGeKQn8FXs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dimensionality Reduction"
      ],
      "metadata": {
        "id": "5fOiE72R8GJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering"
      ],
      "metadata": {
        "id": "pHoUkh_y8MRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ALEX NOTE: INCLUDE IMAGES FOR THE TWO ARCHITECURES**"
      ],
      "metadata": {
        "id": "L3Bte7Xg5CFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifiers\n",
        "\n",
        "One of the things that we tasked ourselves with in Milestone 1 was \"Model Selection.\" In our efforts to find which model's might work best, we did some testing on a couple models. I (Alex S.) have a lot more experience with neural networks than other types of models, so I decided that I would do some research and testing on neural networks for our project. Neural networks are often very good at detecting complex patterns and non-linear relationships, which could potentially be beneficial with our current dataset, as many PA Complexes have similar feature values.\n",
        "\n",
        "We have currently determined two architectures for our model. The first is one in which the \"ML box\" is doing classification, in which it outputs the probability that 1 PA Complex has a higher kd value than another, repeating this process until we have a ranking for all the PA Complexes in our dataset. The other is where the \"ML Box\" does regression, in which it directly outputs the kd values of each PA Complex inputted and then ranks them from least to greatest. Both of these architectures will allow us to accomplish our task, however they each have their own upsides and downsides. By doing classification, this would allow us to have significantly more data to train on, as we would input every possible pair of PA Complexes and their data points into the ML box, whereas for regression we would only input 1 PA Complex at a time into the ML Box. However, if the input features are good enough, the model may be extremely accurate in predicting the exact kd value, thus allowing our ranking to be very accurate as well. The margin of error for the classifcation task may possibly be much larger. For this round of model testing, we tested the second architecure, as shown in the code below. (It is also important to note that we are not doing the actual ranking here, we are just training a model to output the kd value of a PA Complex. To rank them we would just use a sorting algorithm on all the predicted kd values of the test set)\n",
        "\n",
        "In order to \"test\" neural networks for our project, I created a dataset that mimics our own: it consists of 100 PA Complexes, each with a random kd value between 0 and 10, and 20 Properties with a random value between 0 and 100. The Properties resemble our input features. Obviously the actual data will not just be random floats, but this dataset will still help us to see if a neural network will work at all on our actual data. In fact, it will be much easier for a model to learn on non-random data where there are clear patterns in the input features that determine the kd value.\n",
        "\n",
        "The first model that I tested was a simple fully connected neural network with 3 hidden layers. The code and its explanation is attached below."
      ],
      "metadata": {
        "id": "VxXeQUJI8P7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Necessary Dependencies"
      ],
      "metadata": {
        "id": "lXGxAvLe12xF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "sSF4vrzX1tin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create Environment and Set Hyperparameters\n",
        "\n",
        "- Hyperparams: learning rate, number of epochs, loss function, batch"
      ],
      "metadata": {
        "id": "CVGtXnwg1xew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setting basic environment\n",
        "torch.manual_seed(100)\n",
        "\n",
        "#if we have a gpu then run network on that, if not use cpu which is much slower\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#\" Varying learning rate between 0.0001 and 0.01 is considered optimal in most of the cases\" - https://www.kdnuggets.com/2022/12/tuning-adam-optimizer-parameters-pytorch.html\n",
        "lr = 0.001\n",
        "batch_size = 1\n",
        "num_epochs = 801\n",
        "loss_fn = nn.L1Loss()"
      ],
      "metadata": {
        "id": "h26OhR7m153e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create Data\n",
        "\n",
        "- Create a Dataframe that mimics our actual data in that there are 10 \"PA Complexes,\" each of which has a numerical value for 20 different \"Properties.\"\n",
        "\n",
        "- Create a custom dataset for our dataframe, this will allow us to use Pytorch's DataLoader which helps us train\n",
        "\n",
        "- Split data into test, train, val splits\n",
        "\n",
        "- Input our data into the DataLoader"
      ],
      "metadata": {
        "id": "TRhBkqmJ18TQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(48598585)\n",
        "data = {}\n",
        "data.update({\"KD Value\": [rng.random()*10 for x in range(1,101)]})\n",
        "\n",
        "for i in range(1, 21):\n",
        " data.update({\"Property \" + str(i): [rng.random()*100 for x in range(1,101)]})\n",
        "\n",
        "df = pd.DataFrame(data , index = [\"PA Complex \" + str(x) for x in range(1,101)])\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "       self.dataframe = dataframe\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "       row = self.dataframe.iloc[index].to_numpy()\n",
        "       features = row[1:]\n",
        "       label = row[0]\n",
        "       return features, label\n",
        "\n",
        "    def __len__(self):\n",
        "       return len(self.dataframe)\n",
        "\n",
        "train_set = df.iloc[0:51]\n",
        "test_set = df.iloc[51:78]\n",
        "val_set = df.iloc[78:101]\n",
        "data1 = CustomDataset(dataframe=train_set)\n",
        "data2 = CustomDataset(dataframe=test_set)\n",
        "data3 = CustomDataset(dataframe=val_set)\n",
        "train_loader = DataLoader(data1, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(data2, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(data3, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#UNCOMMENT to see what the sample data looks like\n",
        "# print(test_set)\n",
        "# for sample in train_loader:\n",
        "#    print(sample)"
      ],
      "metadata": {
        "id": "W0_R-Wpe1-cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Create Our Model\n",
        "\n",
        "- Create Model class\n",
        "\n",
        "    - def _init__ is the function where we define our model, def forward is where we define our forward pass\n",
        "    \n",
        "    - Use ReLU to only pass positive values through the net, thus activating less neurons during each pass, thus allowing for easier computation. Use Leaky form to avoid dying ReLU problem (sometimes neurons may turn off permanently)\n",
        "    \n",
        "    - Dropout is used after each layer to reduce overfitting: co-adaptation occurs when multiple neurons extract the same\n",
        "      or very similar features, thus emphasizing those features in the result. This can lead to over fitting if those\n",
        "      features are specfic only to the training set. So, dropout randomly shuts down a # of neurons in a layer by\n",
        "      setting their weights to 0. Frac of neurons zeroed out is **r_d**, all other neurons multiplied by **1/1-r_d** so overall sum of\n",
        "      neurons stays the same\n",
        "\n",
        "- Instantiate the object\n",
        "\n",
        "- Define the optimizer for use in training"
      ],
      "metadata": {
        "id": "eM-DGWCS2ARE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(20, 50),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(50, 100),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(100, 200),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(200, 50),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(50, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.model(x)\n",
        "        return output\n",
        "\n",
        "#to run using GPU, we have instantiate as usual but then send to GPU w/ .to() the device\n",
        "simple_NN = Simple_NN().to(device=device)\n",
        "optimizer_SNN = torch.optim.Adam(simple_NN.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "PPMWqgEZ2CRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Training Process\n",
        "- We first create an accuracy function that returns the percentage accuracy, i.e. if the predicted kd is 3, and the true kd is 4, then predicted value is 75% of what we want\n",
        "\n",
        "- Next define train function\n",
        "    - We include 3 seperate blocks for each dataset. The model's weights are updated by training on the train set, and the model's hyperparameters are updated by checking the performance on the validation set for each epoch. We then exit the training loop (when we have gone through every epoch) and evaluate on the test set\n",
        "\n",
        "    - We create 3 graphs, one for the training loss, one for the test loss on each PA Complex (in the test set), and one for the test accuracy on each PA Complex\n",
        "\n",
        "    - We employ a number of functions and methods to train the model:\n",
        "        - Adam Optimizer: Updates weights with optimizer.step()\n",
        "        - Learning Rate Scheduler: decreases the learning rate every 100 epochs to avoid overshooting the minimum via scheduler.step()\n",
        "        - L1 Loss = Mean Absolute Error Loss: Updated via loss.backward()\n",
        "        - Use of model.train() and model.eval() b/c certain things like dropout layers funtion differently in Pytorch depending on if you want to train or evalulate"
      ],
      "metadata": {
        "id": "3R2edgd52F-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#UNCOMMENT to see untrained model predictions on train set\n",
        "# for i, data in enumerate(test_loader):\n",
        "#     values, labels = data\n",
        "#     values = values.to(torch.float32)\n",
        "#     untrained_preds = simple_NN(values)\n",
        "#     print(untrained_preds)\n",
        "#     print(labels)\n",
        "\n",
        "def accuracy_fn(y_true, y_pred):\n",
        "    return y_pred.item() / y_true.item()\n",
        "\n",
        "def train(model, num_epochs, loss_fn, train_data, val_data, test_data, optimizer):\n",
        "    epoch_loss1 = []\n",
        "    epoch_loss2 = []\n",
        "    train_acc = []\n",
        "    val_acc = []\n",
        "    test_acc =[]\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    test_losses = []\n",
        "    scheduler = lr_scheduler.StepLR(optimizer, step_size = 100, gamma=0.9)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        for i, data in enumerate(train_data):\n",
        "            values, label = data\n",
        "            values = values.to(torch.float32)\n",
        "            label = label.to(torch.float32)\n",
        "            optimizer.zero_grad()\n",
        "            train_preds = model(values)\n",
        "            train_loss = loss_fn(train_preds, label)\n",
        "            epoch_loss1.append(train_loss.item())\n",
        "            train_acc.append(accuracy_fn(label, train_preds))\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "        train_losses.append(np.mean(epoch_loss1))\n",
        "\n",
        "        model.eval()\n",
        "        for j, data in enumerate(val_data):\n",
        "            values, label = data\n",
        "            values = values.to(torch.float32)\n",
        "            label = label.to(torch.float32)\n",
        "            val_preds = model(values)\n",
        "            val_loss = loss_fn(val_preds, label)\n",
        "            epoch_loss2.append(val_loss.item())\n",
        "            val_acc.append(accuracy_fn(label, val_preds))\n",
        "        val_losses.append(np.mean(epoch_loss2))\n",
        "\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {np.mean(epoch_loss1):.4f}, Train Accuracy: {np.mean(train_acc):.2f}, Val Loss: {np.mean(val_losses):.4f} Val Accuracy: {np.mean(val_acc):.2f}\")\n",
        "\n",
        "    for k, data in enumerate(test_data):\n",
        "        values, label = data\n",
        "        values = values.to(torch.float32)\n",
        "        label = label.to(torch.float32)\n",
        "        test_preds = model(values)\n",
        "        test_loss = loss_fn(test_preds, label)\n",
        "        test_losses.append(test_loss.item())\n",
        "        test_acc.append(accuracy_fn(label, test_preds))\n",
        "\n",
        "    plt.plot(np.arange(num_epochs), train_losses)\n",
        "    plt.title(\"Training Loss Graph\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(np.arange(27), test_losses)\n",
        "    plt.title(\"Test Loss for Each PA Complex\")\n",
        "    plt.xlabel(\"PA Complex\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.bar(np.arange(27), test_acc)\n",
        "    plt.title(\"Accuracy for Each PA Complex\")\n",
        "    plt.xlabel(\"PA Complex\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.show()\n",
        "\n",
        "train(simple_NN, num_epochs, loss_fn, train_loader, val_loader, test_loader, optimizer_SNN)"
      ],
      "metadata": {
        "id": "BMayLMTR2IJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Model Analysis\n",
        "\n",
        "**TO COMPLETE**"
      ],
      "metadata": {
        "id": "tV2dKdFk5RZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusions"
      ],
      "metadata": {
        "id": "x5R4Ik1a8TO4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G18o433P5XiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References"
      ],
      "metadata": {
        "id": "Cxb_TCI-ya-o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXG3Fw6a8c75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing (Cole)\n",
        "The goal of our data preprocessing was to convert the data into a usable format for training models. The first step was to create our labels based off of the experimental kd values that we have. There are 53 different protein-aptamer complexes that we are looking at, and in order to create labels we need to look at every pair of complexes and determine which one has a lower kd value which is associated with a stronger bond. By framing the problem as a comparison, our number of data points becomes 53*52 = 2757 data points."
      ],
      "metadata": {
        "id": "kmO5WCm1x9L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import pandas as pd\n",
        "import numpy as np'''"
      ],
      "metadata": {
        "id": "eEM_g4DAyEeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def translate_kd(kd):\n",
        "    kd.strip()\n",
        "    kd = kd[3:]\n",
        "    factor = 0\n",
        "    if \"nM\" in kd:\n",
        "        factor = 1\n",
        "    elif \"pM\" in kd:\n",
        "        factor = .001\n",
        "    elif \"uM\" in kd:\n",
        "        factor = 1000\n",
        "    kd = kd[:-2]\n",
        "    try:\n",
        "        return float(kd) * factor\n",
        "    except:\n",
        "        print(kd)\n",
        "        return None\n",
        "\n",
        "def output(df):\n",
        "    df_Final = pd.DataFrame(columns=[\"PDB code 1\",\"Protein Name 1\",\"Ligand Name 1\",\"Affinity Data 1\",\"kd value (nM) 1\",\"PDB code 2\",\"Protein Name 2\",\"Ligand Name 2\",\"Affinity Data 2\",\"kd value (nM) 2\",\"Output\"])\n",
        "    for _ , row1 in df.iterrows():\n",
        "        for _ , row2 in df.iterrows():\n",
        "            if row1[\"PDB code\"] == row2[\"PDB code\"]:\n",
        "                continue\n",
        "            output = 1 if row1[\"kd value (nM)\"] < row2[\"kd value (nM)\"] else 0\n",
        "            temp = row1.to_list() + row2.to_list()\n",
        "            temp.append(output)\n",
        "            df_Final.loc[len(df_Final.index)] = temp\n",
        "    return df_Final'''"
      ],
      "metadata": {
        "id": "dd-U1Soh0Tte"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''df = pd.read_csv(\"KD_data.csv\")\n",
        "df.drop(index = 0,inplace=True)\n",
        "df = df[['PDB code','Protein Name','Ligand Name','Affinity Data']]\n",
        "df[\"kd value (nM)\"] = [translate_kd(kd) for kd in df[\"Affinity Data\"]]\n",
        "df_compare = pd.read_csv(\"Aptamer-Protein-Information.csv\")\n",
        "df = df[df[\"PDB code\"].isin(df_compare[\"Unnamed: 0\"])]\n",
        "df = output(df)\n",
        "df.to_csv(\"data_output.csv\",index=False) '''"
      ],
      "metadata": {
        "id": "qspmqZj70vgC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "dc438b3f-3465-4dbd-c155-f753868a1c35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'KD_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-729ac880dd91>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"KD_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PDB code'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Protein Name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Ligand Name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Affinity Data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kd value (nM)\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtranslate_kd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Affinity Data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_compare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Aptamer-Protein-Information.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KD_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Current Member Contributions:\n",
        "\n",
        "Alex: **Milestone 1]** Took notes on potential projects during thr first meeting. Created and structured the Colab Notebook. Wrote the \"Problem\" section based off of the group's current understanding of the problem we hope to solve. Wrote the Task Organization section with Alan.\n",
        "\n",
        "Jasroop: **Milestone 1]** Participated in discussions for potential project topics. Created Github repo for the project. Read research papers to understand background knowledge for the project.\n",
        "\n",
        "Cole: **Milestone 1]** Contributed to discussion about project ideas. Wrote 'Model Selection Approach' section.\n",
        "\n",
        "Alan: **Milestone 1]** Added to and edited \"Problem\" section. Wrote the \"Similar Problems\" Section. Begun writing and executing python code for feature extraction using GROMACS.\n",
        "\n",
        "Andrea:\n",
        "  - **Milestone 1]**  I participated in and contributed to the meetings by proposing ideas for a project, suggesting ways to conduct feature selection using ANOVA or other techniques to assess statistical significance, and providing sources useful to have a better understanding of the problem and the PDB dataset. I wrote the paragraphs 'Examples', 'Report and Code Organization' and 'Data description', to better organize our objectives for the next phase of the project.\n",
        "  - **(Milestone 1, Milestone 2]** I contributed to the meetings and to the discussions on how to proceed with the project. I converted the cartesian coordinates relative to the protein-aptamer bonds into spherical coordinates. Then I applied KMeans and Spectral clustering, found ten cluster centers and compared the quality of the results using silhouette score. This indicated as preferable the results obtained with KMeans, and the coordinates of the corresponding cluster centers will be used as the most representative angles at which the protein-aptamer bonds form. I reorganized and corrected part of the report from Milestone 1."
      ],
      "metadata": {
        "id": "jn3YUmUIibmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future Tasks Organization:\n",
        "\n",
        "These are the current tasks that we will be working on in the near future. We expect to encounter more tasks as we go along, but this is just what we are dealing with at the moment.\n",
        "\n",
        "Choose Input Features: Alan, Andrea\n",
        "\n",
        "  - Requires physics and chemistry knowledge\n",
        "  - Determine which input features to use in order to accurately find the best protein + aptamer combination\n",
        "\n",
        "Data Preprocessing: Jasroop\n",
        "\n",
        "  - Transfer data into matrix/vector\n",
        "  - Will most likely need to be familar with numpy, SKLearn, and others\n",
        "\n",
        "Model Selection: Alex, Cole\n",
        "\n",
        "  - Two people\n",
        "  - Find which models will be best for our task\n",
        "  - Familiarize with the APIs (Tensorflow? Pytorch? Others?)\n",
        "\n",
        "Data Analysis: Cole\n",
        "\n",
        "  - After the model is chosen, first compare with a random model that randomly matches proteins to aptamers; our model should at the very least beat the random one to start\n",
        "  - Then go on to compare with other models\n"
      ],
      "metadata": {
        "id": "Og5dYvfjn8Ou"
      }
    }
  ]
}